==> startEpoch:  0
in [reward_wait] reward = 0
[in reward_transmit]: input Channel::[0, 0], state:[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0], reward = -8.784233454094307
mean of reward_transmit_list: -8.784233454094307
step in send 0 0
after in send 0 212.0
[in reward_transmit]: input Channel::[0, 0], state:[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0], reward = -8.784233454094307
mean of reward_transmit_list: -8.784233454094307
step in collision 0 212.0
[in reward_transmit]: input Channel::[0, 0], state:[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0], reward = -8.784233454094307
mean of reward_transmit_list: -8.784233454094307
step in collision 0 212.0
[in reward_transmit]: input Channel::[0, 0], state:[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0], reward = -8.784233454094307
mean of reward_transmit_list: -8.784233454094307
step in collision 0 212.0
[in reward_transmit]: input Channel::[0, 2], state:[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 2], reward = -1.0
mean of reward_transmit_list: -7.227386763275446
step in collision 1 212.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[1, 2], state:[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 1 2], reward = -1.0
mean of reward_transmit_list: -6.189488969396205
step in collision 212.0 213.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 3], state:[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1
 2 0 3], reward = -1.0
mean of reward_transmit_list: -5.448133402339605
step in collision 213.0 424.0
[in reward_transmit]: input Channel::[0, 3], state:[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1
 2 0 3], reward = -1.0
mean of reward_transmit_list: -4.892116727047154
step in collision 213.0 425.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 3], state:[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1
 2 0 3], reward = -1.0
mean of reward_transmit_list: -4.459659312930803
step in collision 213.0 425.0
[in reward_transmit]: input Channel::[0, 3], state:[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1
 2 0 3], reward = -1.0
mean of reward_transmit_list: -4.113693381637723
step in collision 214.0 425.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[1, 2], state:[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0
 3 1 2], reward = -1.9
mean of reward_transmit_list: -3.912448528761566
step in collision 425.0 426.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 3], state:[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0 3 1
 2 0 3], reward = -1.9
mean of reward_transmit_list: -3.7447444846981024
step in collision 426.0 637.0
[in reward_transmit]: input Channel::[1, 2], state:[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2 0
 3 1 2], reward = -1.9
mean of reward_transmit_list: -3.602841062798248
step in collision 426.0 638.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0 3 1 2 0
 3 0 2], reward = -1.0
mean of reward_transmit_list: -3.4169238440269445
step in collision 427.0 638.0
[in reward_transmit]: input Channel::[0, 2], state:[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1 2 0 2 0
 2 0 2], reward = -2.71
mean of reward_transmit_list: -3.3697955877584818
step in collision 427.0 639.0
[in reward_transmit]: input Channel::[1, 2], state:[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0 3 1
 2 1 2], reward = -2.71
mean of reward_transmit_list: -3.3285583635235767
step in collision 637.0 639.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[1, 2], state:[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0 3 1 2 0
 3 1 2], reward = -2.71
mean of reward_transmit_list: -3.2921725774339547
step in collision 638.0 849.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2 0 3 1 2 1 2 0
 2 0 2], reward = -2.71
mean of reward_transmit_list: -3.2598296564654015
step in collision 640.0 850.0
[in reward_transmit]: input Channel::[0, 2], state:[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1 2 0 2 0 2 0 2 1
 2 0 2], reward = -3.439
mean of reward_transmit_list: -3.26925967454617
step in collision 640.0 852.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0 3 1 2 0 3 0 2 1 2 0 3 0 2 0 2 0 2 0 2 0
 2 0 2], reward = -5.217031
mean of reward_transmit_list: -3.3666482408188614
step in collision 646.0 852.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[1, 2], state:[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0 3 1 2 0 3 1
 2 1 2], reward = -3.439
mean of reward_transmit_list: -3.3700935626846293
step in collision 850.0 858.0
[in reward_transmit]: input Channel::[0, 2], state:[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0 3 1 2 1 2 1
 2 0 2], reward = -1.0
mean of reward_transmit_list: -3.2623620371080553
step in collision 850.0 1062.0
[in reward_transmit]: input Channel::[1, 2], state:[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2 0 3 1 2 1 2 0 2 0
 2 1 2], reward = -3.439
mean of reward_transmit_list: -3.2700419485381396
step in collision 852.0 1062.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1 2 0 2 0 2 0 2 1 2 0 2 1 2 0 2 0
 2 0 2], reward = -5.217031
mean of reward_transmit_list: -3.351166492349051
step in collision 855.0 1064.0
[in reward_transmit]: input Channel::[1, 2], state:[0 0 0 0 0 0 0 0 0 0 0 0 1 2 0 3 1 2 0 3 0 2 1 2 0 3 0 2 0 2 0 2 0 2 0 2 0
 2 1 2], reward = -3.439
mean of reward_transmit_list: -3.354679832655089
step in collision 858.0 1067.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[1, 2], state:[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0 3 1 2 0 3 1 2 1
 2 1 2], reward = -4.0951
mean of reward_transmit_list: -3.383157531399124
step in collision 1062.0 1070.0
[in reward_transmit]: input Channel::[0, 2], state:[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0 3 1 2 1 2 1 2 0 2 1
 2 0 2], reward = -1.9
mean of reward_transmit_list: -3.3282257709769345
step in collision 1063.0 1274.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[1, 2], state:[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1 2 0 2 0 2 0 2 1 2 0 2 1 2 0 2 0 2 0
 2 1 2], reward = -4.0951
mean of reward_transmit_list: -3.355614136299187
step in collision 1067.0 1275.0
[in reward_transmit]: input Channel::[0, 2], state:[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2 0 3 1 2 1 2 0 2 0 2 1 2 1 2 0 2 0
 2 0 2], reward = -4.68559
mean of reward_transmit_list: -3.4014753729785254
step in collision 1067.0 1279.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[tensor(0, device='cuda:0'), 2], state:[0 0 1 2 0 3 1 2 0 3 0 2 1 2 0 3 0 2 0 2 0 2 0 2 0 2 0 2 1 2 1 2 0 2 0 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2], reward = -6.861894039100001
mean of reward_transmit_list: -3.516822661849241
step in collision 1074.0 1279.0
[in reward_transmit]: input Channel::[1, 2], state:[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0 3 1 2 0 3 1 2 1 2 1
 2 1 2], reward = -4.68559
mean of reward_transmit_list: -3.554524834047653
step in collision 1274.0 1286.0
[in reward_transmit]: input Channel::[1, 2], state:[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0 3 1 2 1 2 1 2 0 2 1 2 0
 2 1 2], reward = -4.68559
mean of reward_transmit_list: -3.589870620483664
step in collision 1275.0 1486.0
[in reward_transmit]: input Channel::[1, 2], state:[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2 0 3 1 2 1 2 0 2 0 2 1 2 1 2 0 2 0 2 0
 2 1 2], reward = -4.68559
mean of reward_transmit_list: -3.623074238044765
step in collision 1279.0 1487.0
[in reward_transmit]: input Channel::[1, 2], state:[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 1 2 0 2 0 2 0 2 1 2 0 2 1 2 0 2 0 2 0 2 1
 2 1 2], reward = -4.68559
mean of reward_transmit_list: -3.6543247016316838
step in collision 1279.0 1491.0
[in reward_transmit]: input Channel::[1, 2], state:[1 2 0 3 1 2 0 3 0 2 1 2 0 3 0 2 0 2 0 2 0 2 0 2 0 2 1 2 1 2 0 2 0 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 1 2], reward = -4.68559
mean of reward_transmit_list: -3.683789424442207
step in collision 1286.0 1491.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[1, 2], state:[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0 3 1 2 1 2 1 2 0 2 1 2 0 2 1
 2 1 2], reward = -5.217031
mean of reward_transmit_list: -3.726379468207701
step in collision 1487.0 1498.0
[in reward_transmit]: input Channel::[0, 2], state:[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0 3 1 2 0 3 1 2 1 2 1 2 1 2 1
 2 0 2], reward = -1.0
mean of reward_transmit_list: -3.65269353663452
step in collision 1487.0 1699.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[1, 2], state:[0 0 0 0 0 0 0 0 0 0 0 0 1 2 1 2 0 2 0 2 0 2 1 2 0 2 1 2 0 2 0 2 0 2 1 2 1
 2 1 2], reward = -5.217031
mean of reward_transmit_list: -3.693860311986243
step in collision 1491.0 1699.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[0 0 0 0 0 0 0 2 1 2 0 3 1 2 1 2 0 2 0 2 1 2 1 2 0 2 0 2 0 2 1 2 1 2 0 2 0
 2 0 2], reward = -6.12579511
mean of reward_transmit_list: -3.7562176144994166
step in collision 1494.0 1703.0
[in reward_transmit]: input Channel::[1, 2], state:[0 3 1 2 0 3 0 2 1 2 0 3 0 2 0 2 0 2 0 2 0 2 0 2 1 2 1 2 0 2 0 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 1 2 1 2], reward = -4.68559
mean of reward_transmit_list: -3.779451924136931
step in collision 1498.0 1706.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[1, 2], state:[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0 3 1 2 1 2 1 2 0 2 1 2 0 2 1 2 1
 2 1 2], reward = -5.6953279000000006
mean of reward_transmit_list: -3.826180606475054
step in collision 1699.0 1710.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0 3 1 2 0 3 1 2 1 2 1 2 1 2 1 2 0 2 1 2
 tensor(0, device='cuda:0') 2 0 2], reward = -2.71
mean of reward_transmit_list: -3.799604877749458
step in collision 1701.0 1911.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[0 0 0 0 1 2 1 2 0 2 0 2 0 2 1 2 0 2 1 2 0 2 0 2 0 2 1 2 1 2 1 2 1 2 0 2 0
 2 0 2], reward = -6.5132155990000005
mean of reward_transmit_list: -3.8627121038250523
step in collision 1706.0 1913.0
[in reward_transmit]: input Channel::[1, 2], state:[0 0 0 0 0 2 1 2 0 3 1 2 1 2 0 2 0 2 1 2 1 2 0 2 0 2 0 2 1 2 1 2 0 2 0 2 0
 2 1 2], reward = -5.6953279000000006
mean of reward_transmit_list: -3.904362462829028
step in collision 1706.0 1918.0
[in reward_transmit]: input Channel::[1, 2], state:[1 2 0 3 0 2 1 2 0 3 0 2 0 2 0 2 0 2 0 2 0 2 1 2 1 2 0 2 0 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 1 2 1 2 1 2], reward = -5.217031
mean of reward_transmit_list: -3.9335328747661604
step in collision 1710.0 1918.0
[in reward_transmit]: input Channel::[1, 2], state:[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0 3 1 2 1 2 1 2 0 2 1 2 0 2 1 2 1 2 1
 2 1 2], reward = -6.12579511
mean of reward_transmit_list: -3.9811907494451573
step in collision 1911.0 1922.0
[in reward_transmit]: input Channel::[1, 2], state:[0 0 0 0 0 0 0 0 0 0 0 0 1 2 0 3 1 2 0 3 1 2 1 2 1 2 1 2 1 2 0 2 1 2
 tensor(0, device='cuda:0') 2 0 2 1 2], reward = -6.12579511
mean of reward_transmit_list: -4.026820629456963
step in collision 1913.0 2123.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[1, 2], state:[0 0 1 2 1 2 0 2 0 2 0 2 1 2 0 2 1 2 0 2 0 2 0 2 1 2 1 2 1 2 1 2 0 2 0 2 0
 2 1 2], reward = -6.12579511
mean of reward_transmit_list: -4.070549264468276
step in collision 1918.0 2125.0
[in reward_transmit]: input Channel::[0, 2], state:[0 2 1 2 0 3 1 2 1 2 0 2 0 2 1 2 1 2 0 2 0 2 0 2 1 2 1 2 0 2 0 2 0 2 1 2 1
 2 0 2], reward = -6.5132155990000005
mean of reward_transmit_list: -4.120399597826067
step in collision 1919.0 2130.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[tensor(0, device='cuda:0'), 2], state:[0 3 0 2 0 2 0 2 0 2 0 2 0 2 1 2 1 2 0 2 0 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 1 2 0 2 0 2
 tensor(0, device='cuda:0') 2], reward = -7.458134171671
mean of reward_transmit_list: -4.187154289302965
step in collision 1925.0 2131.0
[in reward_transmit]: input Channel::[1, 2], state:[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0 3 1 2 1 2 1 2 0 2 1 2 0 2 1 2 1 2 1 2 1
 2 1 2], reward = -6.5132155990000005
mean of reward_transmit_list: -4.232763334591143
step in collision 2123.0 2137.0
[in reward_transmit]: input Channel::[tensor(1, device='cuda:0'), 2], state:[0 0 0 0 0 0 0 0 0 0 1 2 0 3 1 2 0 3 1 2 1 2 1 2 1 2 1 2 0 2 1 2
 tensor(0, device='cuda:0') 2 0 2 1 2 tensor(1, device='cuda:0') 2], reward = -6.5132155990000005
mean of reward_transmit_list: -4.276618185829775
step in collision 2125.0 2335.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[1, 2], state:[1 2 0 3 1 2 1 2 0 2 0 2 1 2 1 2 0 2 0 2 0 2 1 2 1 2 0 2 0 2 0 2 1 2 1 2 0
 2 1 2], reward = -6.5132155990000005
mean of reward_transmit_list: -4.318818137021665
step in collision 2131.0 2337.0
[in reward_transmit]: input Channel::[0, 2], state:[0 2 0 2 0 2 1 2 0 2 1 2 0 2 0 2 0 2 1 2 1 2 1 2 1 2 0 2 0 2 0 2 1 2 1 2 0
 2 0 2], reward = -7.175704635190001
mean of reward_transmit_list: -4.371723442543301
step in collision 2132.0 2343.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[tensor(0, device='cuda:0'), 2], state:[0 2 0 2 0 2 0 2 1 2 1 2 0 2 0 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 1 2 0 2 0 2
 tensor(0, device='cuda:0') 2 1 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2], reward = -7.458134171671
mean of reward_transmit_list: -4.427840001254713
step in collision 2139.0 2344.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[0 0 0 0 0 0 1 2 0 3 1 2 0 3 1 2 1 2 1 2 1 2 1 2 0 2 1 2
 tensor(0, device='cuda:0') 2 0 2 1 2 tensor(1, device='cuda:0') 2 1 2 0 2], reward = -3.439
mean of reward_transmit_list: -4.410182144089451
step in collision 2338.0 2351.0
[in reward_transmit]: input Channel::[0, 2], state:[0 0 0 0 1 2 0 3 1 2 1 2 1 2 0 2 1 2 0 2 1 2 1 2 1 2 1 2 1 2 1 2 0 2 0 2 0
 2 0 2], reward = -4.68559
mean of reward_transmit_list: -4.415013860859811
step in collision 2339.0 2550.0
[in reward_transmit]: input Channel::[1, 2], state:[0 3 1 2 1 2 0 2 0 2 1 2 1 2 0 2 0 2 0 2 1 2 1 2 0 2 0 2 0 2 1 2 1 2 0 2 1
 2 1 2], reward = -6.5132155990000005
mean of reward_transmit_list: -4.451189752896711
step in collision 2343.0 2551.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[0 2 1 2 0 2 1 2 0 2 0 2 0 2 1 2 1 2 1 2 1 2 0 2 0 2 0 2 1 2 1 2 0 2 0 2 1
 2 0 2], reward = -6.861894039100001
mean of reward_transmit_list: -4.492049147578122
step in collision 2345.0 2555.0
[in reward_transmit]: input Channel::[1, 2], state:[0 2 0 2 0 2 1 2 1 2 0 2 0 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 1 2 0 2 0 2
 tensor(0, device='cuda:0') 2 1 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 1 2], reward = -5.6953279000000006
mean of reward_transmit_list: -4.5121037934518196
step in collision 2351.0 2557.0
[in reward_transmit]: input Channel::[1, 2], state:[0 0 0 0 1 2 0 3 1 2 0 3 1 2 1 2 1 2 1 2 1 2 0 2 1 2
 tensor(0, device='cuda:0') 2 0 2 1 2 tensor(1, device='cuda:0') 2 1 2 0 2
 1 2], reward = -7.175704635190001
mean of reward_transmit_list: -4.555769381021299
step in collision 2550.0 2563.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[tensor(0, device='cuda:0'), 2], state:[1 2 0 3 1 2 1 2 1 2 0 2 1 2 0 2 1 2 1 2 1 2 1 2 1 2 1 2 0 2 0 2 0 2 0 2 1
 2 tensor(0, device='cuda:0') 2], reward = -5.217031
mean of reward_transmit_list: -4.5664348910048265
step in collision 2552.0 2762.0
[in reward_transmit]: input Channel::[1, 2], state:[1 2 1 2 0 2 0 2 1 2 1 2 0 2 0 2 0 2 1 2 1 2 0 2 0 2 0 2 1 2 1 2 0 2 1 2 1
 2 1 2], reward = -6.861894039100001
mean of reward_transmit_list: -4.6028707504984006
step in collision 2555.0 2764.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[0 2 1 2 0 2 0 2 0 2 1 2 1 2 1 2 1 2 0 2 0 2 0 2 1 2 1 2 0 2 0 2 1 2 0 2 1
 2 0 2], reward = -6.861894039100001
mean of reward_transmit_list: -4.6381679893828
step in collision 2558.0 2767.0
[in reward_transmit]: input Channel::[1, 2], state:[0 2 0 2 1 2 1 2 0 2 0 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 1 2 0 2 0 2
 tensor(0, device='cuda:0') 2 1 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 1 2 1 2], reward = -6.12579511
mean of reward_transmit_list: -4.661054560469219
step in collision 2563.0 2770.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[1, 2], state:[0 3 1 2 1 2 1 2 0 2 1 2 0 2 1 2 1 2 1 2 1 2 1 2 1 2 0 2 0 2 0 2 0 2 1 2
 tensor(0, device='cuda:0') 2 1 2], reward = -7.175704635190001
mean of reward_transmit_list: -4.699155319177109
step in collision 2764.0 2775.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[1 2 1 2 1 2 1 2 1 2 0 2 1 2 tensor(0, device='cuda:0') 2 0 2 1 2
 tensor(1, device='cuda:0') 2 1 2 0 2 1 2 1 2 0 2 0 2 0 2 0 2 0 2], reward = -6.12579511
mean of reward_transmit_list: -4.720448450383421
step in collision 2767.0 2976.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[tensor(0, device='cuda:0'), 2], state:[0 2 0 2 1 2 1 2 0 2 0 2 0 2 1 2 1 2 0 2 0 2 0 2 1 2 1 2 0 2 1 2 1 2 1 2 1
 2 tensor(0, device='cuda:0') 2], reward = -6.5132155990000005
mean of reward_transmit_list: -4.746812673157194
step in collision 2768.0 2979.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[1 2 1 2 1 2 1 2 0 2 0 2 0 2 1 2 1 2 0 2 0 2 1 2 0 2 1 2 0 2 1 2 0 2
 tensor(0, device='cuda:0') 2 0 2 0 2], reward = -6.861894039100001
mean of reward_transmit_list: -4.7774660262868
step in collision 2774.0 2980.0
[in reward_transmit]: input Channel::[1, 2], state:[0 2 1 2 1 2 0 2 0 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 1 2 0 2 0 2
 tensor(0, device='cuda:0') 2 1 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 1 2 1 2 1 2], reward = -6.5132155990000005
mean of reward_transmit_list: -4.8022624487541306
step in collision 2775.0 2986.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[0 2 1 2 0 2 1 2 1 2 1 2 1 2 1 2 1 2 0 2 0 2 0 2 0 2 1 2
 tensor(0, device='cuda:0') 2 1 2 1 2 tensor(0, device='cuda:0') 2 0 2 0 2], reward = -6.5132155990000005
mean of reward_transmit_list: -4.826360380447734
step in collision 2979.0 2987.0
[in reward_transmit]: input Channel::[tensor(1, device='cuda:0'), 2], state:[1 2 1 2 1 2 1 2 0 2 1 2 tensor(0, device='cuda:0') 2 0 2 1 2
 tensor(1, device='cuda:0') 2 1 2 0 2 1 2 1 2 0 2 0 2 0 2 0 2 0 2
 tensor(1, device='cuda:0') 2], reward = -6.861894039100001
mean of reward_transmit_list: -4.854631681262349
step in collision 2979.0 3191.0
[in reward_transmit]: input Channel::[1, 2], state:[0 2 1 2 1 2 0 2 0 2 0 2 1 2 1 2 0 2 0 2 0 2 1 2 1 2 0 2 1 2 1 2 1 2 1 2
 tensor(0, device='cuda:0') 2 1 2], reward = -6.861894039100001
mean of reward_transmit_list: -4.8821284258902615
step in collision 2980.0 3191.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[tensor(0, device='cuda:0'), 2], state:[1 2 0 2 0 2 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 1 2
 1 2 1 2 1 2 0 2 0 2 tensor(0, device='cuda:0') 2 1 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 1 2
 tensor(0, device='cuda:0') 2], reward = -6.5132155990000005
mean of reward_transmit_list: -4.904170144445798
step in collision 2988.0 3192.0
[in reward_transmit]: input Channel::[0, 2], state:[1 2 0 2 0 2 0 2 1 2 1 2 0 2 0 2 1 2 0 2 1 2 0 2 1 2 0 2
 tensor(0, device='cuda:0') 2 0 2 0 2 1 2 tensor(0, device='cuda:0') 2 0 2], reward = -7.458134171671
mean of reward_transmit_list: -4.938222998142135
step in collision 2988.0 3200.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[1, 2], state:[1 2 0 2 1 2 1 2 1 2 1 2 1 2 1 2 0 2 0 2 0 2 0 2 1 2
 tensor(0, device='cuda:0') 2 1 2 1 2 tensor(0, device='cuda:0') 2 0 2 0 2
 1 2], reward = -6.861894039100001
mean of reward_transmit_list: -4.963534459207369
step in collision 3191.0 3200.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[1 2 0 2 0 2 0 2 1 2 1 2 0 2 0 2 0 2 1 2 1 2 0 2 1 2 1 2 1 2 1 2
 tensor(0, device='cuda:0') 2 1 2 1 2 0 2], reward = -6.12579511
mean of reward_transmit_list: -4.978628753373508
step in collision 3193.0 3403.0
[in reward_transmit]: input Channel::[0, 2], state:[0 2 1 2 tensor(0, device='cuda:0') 2 0 2 1 2 tensor(1, device='cuda:0') 2
 1 2 0 2 1 2 1 2 0 2 0 2 0 2 0 2 0 2 tensor(1, device='cuda:0') 2 1 2 0 2
 0 2 0 2], reward = -7.175704635190001
mean of reward_transmit_list: -5.006796392883976
step in collision 3194.0 3405.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[1, 2], state:[0 2 0 2 0 2 1 2 1 2 0 2 0 2 1 2 0 2 1 2 0 2 1 2 0 2
 tensor(0, device='cuda:0') 2 0 2 0 2 1 2 tensor(0, device='cuda:0') 2 0 2
 1 2], reward = -5.217031
mean of reward_transmit_list: -5.009457590442406
step in collision 3200.0 3406.0
[in reward_transmit]: input Channel::[tensor(0, device='cuda:0'), 2], state:[0 2 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 1 2 1 2 1 2
 1 2 0 2 0 2 tensor(0, device='cuda:0') 2 1 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 1 2 tensor(0, device='cuda:0') 2
 1 2 tensor(0, device='cuda:0') 2], reward = -6.5132155990000005
mean of reward_transmit_list: -5.028254565549377
step in collision 3201.0 3412.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[1, 2], state:[0 2 0 2 0 2 1 2 1 2 0 2 0 2 0 2 1 2 1 2 0 2 1 2 1 2 1 2 1 2
 tensor(0, device='cuda:0') 2 1 2 1 2 0 2 1 2], reward = -6.861894039100001
mean of reward_transmit_list: -5.050892089914199
step in collision 3405.0 3413.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[tensor(0, device='cuda:0') 2 0 2 1 2 tensor(1, device='cuda:0') 2 1 2 0 2
 1 2 1 2 0 2 0 2 0 2 0 2 0 2 tensor(1, device='cuda:0') 2 1 2 0 2 0 2 0 2
 1 2 0 2], reward = -7.175704635190001
mean of reward_transmit_list: -5.0768044380273185
step in collision 3407.0 3617.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[tensor(0, device='cuda:0'), 2], state:[1 2 1 2 0 2 0 2 0 2 0 2 1 2 tensor(0, device='cuda:0') 2 1 2 1 2
 tensor(0, device='cuda:0') 2 0 2 0 2 1 2 1 2 0 2 0 2 0 2 0 2
 tensor(0, device='cuda:0') 2], reward = -7.458134171671
mean of reward_transmit_list: -5.105495157709773
step in collision 3408.0 3619.0
[in reward_transmit]: input Channel::[1, 2], state:[0 2 0 2 1 2 1 2 0 2 0 2 1 2 0 2 1 2 0 2 1 2 0 2
 tensor(0, device='cuda:0') 2 0 2 0 2 1 2 tensor(0, device='cuda:0') 2 0 2
 1 2 1 2], reward = -5.6953279000000006
mean of reward_transmit_list: -5.112516976070371
step in collision 3412.0 3620.0
[in reward_transmit]: input Channel::[1, 2], state:[tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 1 2
 0 2 0 2 tensor(0, device='cuda:0') 2 1 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 1 2 tensor(0, device='cuda:0') 2
 1 2 tensor(0, device='cuda:0') 2 1 2], reward = -6.861894039100001
mean of reward_transmit_list: -5.133097882694249
step in collision 3413.0 3624.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[1, 2], state:[0 2 1 2 tensor(1, device='cuda:0') 2 1 2 0 2 1 2 1 2 0 2 0 2 0 2 0 2 0 2
 tensor(1, device='cuda:0') 2 1 2 0 2 0 2 0 2 1 2 0 2 1 2], reward = -6.12579511
mean of reward_transmit_list: -5.144640873709432
step in collision 3619.0 3625.0
[in reward_transmit]: input Channel::[1, 2], state:[1 2 0 2 0 2 0 2 0 2 1 2 tensor(0, device='cuda:0') 2 1 2 1 2
 tensor(0, device='cuda:0') 2 0 2 0 2 1 2 1 2 0 2 0 2 0 2 0 2
 tensor(0, device='cuda:0') 2 1 2], reward = -5.217031
mean of reward_transmit_list: -5.145472944126564
step in collision 3620.0 3831.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[0 2 1 2 1 2 0 2 1 2 1 2 1 2 1 2 tensor(0, device='cuda:0') 2 1 2 1 2 0 2
 1 2 1 2 0 2 0 2 0 2 0 2 0 2 0 2], reward = -6.5132155990000005
mean of reward_transmit_list: -5.1610154742955805
step in collision 3623.0 3832.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[1, 2], state:[tensor(0, device='cuda:0') 2 1 2 1 2 1 2 1 2 0 2 0 2
 tensor(0, device='cuda:0') 2 1 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 1 2 tensor(0, device='cuda:0') 2
 1 2 tensor(0, device='cuda:0') 2 1 2 1 2], reward = -7.175704635190001
mean of reward_transmit_list: -5.1836524311595635
step in collision 3625.0 3835.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[0 2 1 2 0 2 tensor(0, device='cuda:0') 2 0 2 0 2 1 2
 tensor(0, device='cuda:0') 2 0 2 1 2 1 2 1 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2 0 2 0 2], reward = -7.94108867905351
mean of reward_transmit_list: -5.214290611691718
step in collision 3632.0 3837.0
[in reward_transmit]: input Channel::[1, 2], state:[1 2 tensor(1, device='cuda:0') 2 1 2 0 2 1 2 1 2 0 2 0 2 0 2 0 2 0 2
 tensor(1, device='cuda:0') 2 1 2 0 2 0 2 0 2 1 2 0 2 1 2 1 2], reward = -6.5132155990000005
mean of reward_transmit_list: -5.2285645126511495
step in collision 3831.0 3844.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[0 2 0 2 1 2 tensor(0, device='cuda:0') 2 1 2 1 2
 tensor(0, device='cuda:0') 2 0 2 0 2 1 2 1 2 0 2 0 2 0 2 0 2
 tensor(0, device='cuda:0') 2 1 2 1 2 0 2 0 2], reward = -7.458134171671
mean of reward_transmit_list: -5.2527989654665825
step in collision 3834.0 4043.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[tensor(0, device='cuda:0'), 2], state:[1 2 0 2 1 2 1 2 1 2 1 2 tensor(0, device='cuda:0') 2 1 2 1 2 0 2 1 2 1 2
 0 2 0 2 0 2 0 2 0 2 0 2 1 2 tensor(0, device='cuda:0') 2], reward = -6.5132155990000005
mean of reward_transmit_list: -5.266351832493823
step in collision 3836.0 4046.0
[in reward_transmit]: input Channel::[1, 2], state:[1 2 1 2 1 2 1 2 0 2 0 2 tensor(0, device='cuda:0') 2 1 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 1 2
 tensor(0, device='cuda:0') 2 1 2 tensor(0, device='cuda:0') 2 1 2 1 2 1 2], reward = -7.458134171671
mean of reward_transmit_list: -5.289668665889326
step in collision 3837.0 4048.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[0 2 1 2 tensor(0, device='cuda:0') 2 0 2 1 2 1 2 1 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 0 2 0 2 0 2 1 2 0 2
 tensor(0, device='cuda:0') 2 0 2 0 2], reward = -7.94108867905351
mean of reward_transmit_list: -5.317578350238422
step in collision 3848.0 4049.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[1 2 0 2 1 2 1 2 0 2 0 2 0 2 0 2 0 2 tensor(1, device='cuda:0') 2 1 2 0 2
 0 2 0 2 1 2 0 2 1 2 1 2 1 2 0 2], reward = -6.861894039100001
mean of reward_transmit_list: -5.333664971997397
step in collision 4044.0 4060.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[1, 2], state:[1 2 1 2 1 2 0 2 0 2 tensor(0, device='cuda:0') 2 1 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 1 2
 tensor(0, device='cuda:0') 2 1 2 tensor(0, device='cuda:0') 2 1 2 1 2 1 2
 1 2], reward = -7.458134171671
mean of reward_transmit_list: -5.355566716323929
step in collision 4049.0 4256.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[tensor(0, device='cuda:0'), 2], state:[1 2 tensor(0, device='cuda:0') 2 0 2 0 2 1 2 1 2 0 2 0 2 0 2 0 2
 tensor(0, device='cuda:0') 2 1 2 1 2 0 2 0 2 1 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2
 tensor(0, device='cuda:0') 2], reward = -7.7123207545039
mean of reward_transmit_list: -5.379615226917602
step in collision 4050.0 4261.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[1 2 tensor(0, device='cuda:0') 2 1 2 1 2 0 2 1 2 1 2 0 2 0 2 0 2 0 2 0 2
 0 2 1 2 tensor(0, device='cuda:0') 2 1 2 tensor(0, device='cuda:0') 2 0 2
 0 2 0 2], reward = -7.458134171671
mean of reward_transmit_list: -5.400610367773696
step in collision 4052.0 4262.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[tensor(0, device='cuda:0') 2 0 2 1 2 1 2 1 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2 0 2 0 2 1 2
 0 2 tensor(0, device='cuda:0') 2 0 2 0 2 1 2 0 2], reward = -7.94108867905351
mean of reward_transmit_list: -5.426015150886496
step in collision 4061.0 4264.0
[in reward_transmit]: input Channel::[1, 2], state:[0 2 1 2 1 2 0 2 0 2 0 2 0 2 0 2 tensor(1, device='cuda:0') 2 1 2 0 2 0 2
 0 2 1 2 0 2 1 2 1 2 1 2 0 2 1 2], reward = -6.12579511
mean of reward_transmit_list: -5.4329436653331635
step in collision 4256.0 4273.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[0 2 0 2 tensor(0, device='cuda:0') 2 1 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 1 2 tensor(0, device='cuda:0') 2
 1 2 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 1 2 1 2
 tensor(0, device='cuda:0') 2 0 2], reward = -6.12579511
mean of reward_transmit_list: -5.439736326555388
step in collision 4263.0 4468.0
[in reward_transmit]: input Channel::[0, 2], state:[0 2 0 2 1 2 1 2 0 2 0 2 0 2 0 2 tensor(0, device='cuda:0') 2 1 2 1 2 0 2
 0 2 1 2 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2
 tensor(0, device='cuda:0') 2 1 2 0 2], reward = -7.7123207545039
mean of reward_transmit_list: -5.461800253040325
step in collision 4263.0 4475.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[tensor(0, device='cuda:0'), 2], state:[0 2 1 2 1 2 0 2 0 2 0 2 0 2 0 2 0 2 1 2 tensor(0, device='cuda:0') 2 1 2
 tensor(0, device='cuda:0') 2 0 2 0 2 0 2 1 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2], reward = -7.94108867905351
mean of reward_transmit_list: -5.485639564828914
step in collision 4267.0 4475.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[1 2 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 0 2 0 2 0 2 1 2 0 2
 tensor(0, device='cuda:0') 2 0 2 0 2 1 2 0 2 1 2 0 2 0 2 0 2], reward = -8.14697981114816
mean of reward_transmit_list: -5.510985662412907
step in collision 4276.0 4479.0
[in reward_transmit]: input Channel::[1, 2], state:[1 2 1 2 0 2 0 2 0 2 0 2 0 2 tensor(1, device='cuda:0') 2 1 2 0 2 0 2 0 2
 1 2 0 2 1 2 1 2 1 2 0 2 1 2 1 2], reward = -6.5132155990000005
mean of reward_transmit_list: -5.520440661814672
step in collision 4468.0 4488.0
[in reward_transmit]: input Channel::[1, 2], state:[0 2 1 2 1 2 0 2 0 2 0 2 0 2 tensor(0, device='cuda:0') 2 1 2 1 2 0 2 0 2
 1 2 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2
 tensor(0, device='cuda:0') 2 1 2 0 2 1 2], reward = -5.217031
mean of reward_transmit_list: -5.517605057498647
step in collision 4475.0 4680.0
[in reward_transmit]: input Channel::[1, 2], state:[0 2 tensor(0, device='cuda:0') 2 1 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 1 2 tensor(0, device='cuda:0') 2
 1 2 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 1 2 1 2
 tensor(0, device='cuda:0') 2 0 2 1 2], reward = -7.175704635190001
mean of reward_transmit_list: -5.53295783136616
step in collision 4475.0 4687.0
[in reward_transmit]: input Channel::[1, 2], state:[1 2 1 2 0 2 0 2 0 2 0 2 0 2 0 2 1 2 tensor(0, device='cuda:0') 2 1 2
 tensor(0, device='cuda:0') 2 0 2 0 2 0 2 1 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 1 2], reward = -4.68559
mean of reward_transmit_list: -5.5251838145646355
step in collision 4479.0 4687.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[0 2 0 2 1 2 0 2 tensor(0, device='cuda:0') 2 0 2 0 2 1 2 0 2 1 2 0 2 0 2
 0 2 1 2 0 2 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2
 0 2 0 2], reward = -8.14697981114816
mean of reward_transmit_list: -5.549018323624486
step in collision 4494.0 4691.0
[in reward_transmit]: input Channel::[1, 2], state:[1 2 0 2 0 2 0 2 0 2 0 2 tensor(1, device='cuda:0') 2 1 2 0 2 0 2 0 2 1 2
 0 2 1 2 1 2 1 2 0 2 1 2 1 2 1 2], reward = -6.5132155990000005
mean of reward_transmit_list: -5.557704785564805
step in collision 4680.0 4706.0
[in reward_transmit]: input Channel::[1, 2], state:[tensor(0, device='cuda:0') 2 1 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 1 2 tensor(0, device='cuda:0') 2
 1 2 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 1 2 1 2
 tensor(0, device='cuda:0') 2 0 2 1 2 1 2], reward = -7.458134171671
mean of reward_transmit_list: -5.57467290508361
step in collision 4687.0 4892.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[tensor(0, device='cuda:0'), 2], state:[0 2 0 2 0 2 tensor(0, device='cuda:0') 2 1 2 1 2 0 2 0 2 1 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2
 tensor(0, device='cuda:0') 2 1 2 0 2 1 2 1 2 0 2 0 2
 tensor(0, device='cuda:0') 2], reward = -7.7123207545039
mean of reward_transmit_list: -5.5935901426891
step in collision 4690.0 4899.0
[in reward_transmit]: input Channel::[1, 2], state:[1 2 0 2 0 2 0 2 0 2 0 2 0 2 1 2 tensor(0, device='cuda:0') 2 1 2
 tensor(0, device='cuda:0') 2 0 2 0 2 0 2 1 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 1 2 1 2], reward = -4.68559
mean of reward_transmit_list: -5.58562522915674
step in collision 4691.0 4902.0
[in reward_transmit]: input Channel::[1, 2], state:[0 2 1 2 0 2 tensor(0, device='cuda:0') 2 0 2 0 2 1 2 0 2 1 2 0 2 0 2 0 2
 1 2 0 2 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2 0 2
 0 2 1 2], reward = -4.0951
mean of reward_transmit_list: -5.572664140207551
step in collision 4706.0 4903.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[0 2 0 2 0 2 tensor(1, device='cuda:0') 2 1 2 0 2 0 2 0 2 1 2 0 2 1 2 1 2
 1 2 0 2 1 2 1 2 1 2 1 2 0 2 0 2], reward = -6.5132155990000005
mean of reward_transmit_list: -5.58077234243852
step in collision 4894.0 4918.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 1 2
 tensor(0, device='cuda:0') 2 1 2 tensor(0, device='cuda:0') 2 1 2 1 2 1 2
 1 2 1 2 tensor(0, device='cuda:0') 2 0 2 1 2 1 2 1 2 0 2], reward = -5.217031
mean of reward_transmit_list: -5.577663442075798
step in collision 4900.0 5106.0
[in reward_transmit]: input Channel::[1, 2], state:[0 2 0 2 tensor(0, device='cuda:0') 2 1 2 1 2 0 2 0 2 1 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2
 tensor(0, device='cuda:0') 2 1 2 0 2 1 2 1 2 0 2 0 2
 tensor(0, device='cuda:0') 2 1 2], reward = -5.217031
mean of reward_transmit_list: -5.574607234939562
step in collision 4902.0 5112.0
[in reward_transmit]: input Channel::[1, 2], state:[0 2 0 2 0 2 0 2 0 2 0 2 1 2 tensor(0, device='cuda:0') 2 1 2
 tensor(0, device='cuda:0') 2 0 2 0 2 0 2 1 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 1 2 1 2 1 2], reward = -4.68559
mean of reward_transmit_list: -5.567136501872844
step in collision 4903.0 5114.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[tensor(0, device='cuda:0'), 2], state:[0 2 tensor(0, device='cuda:0') 2 0 2 0 2 1 2 0 2 1 2 0 2 0 2 0 2 1 2 0 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2 0 2 0 2 1 2
 1 2 tensor(0, device='cuda:0') 2], reward = -7.94108867905351
mean of reward_transmit_list: -5.586919436682683
step in collision 4919.0 5115.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[1 2 0 2 0 2 0 2 1 2 0 2 1 2 1 2 1 2 0 2 1 2 1 2 1 2 1 2 0 2 0 2
 tensor(1, device='cuda:0') 2 0 2 0 2 0 2], reward = -6.5132155990000005
mean of reward_transmit_list: -5.594574776867123
step in collision 5109.0 5131.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[1, 2], state:[0 2 tensor(0, device='cuda:0') 2 1 2 1 2 0 2 0 2 1 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2
 tensor(0, device='cuda:0') 2 1 2 0 2 1 2 1 2 0 2 0 2
 tensor(0, device='cuda:0') 2 1 2 1 2], reward = -5.6953279000000006
mean of reward_transmit_list: -5.595400622138705
step in collision 5114.0 5321.0
[in reward_transmit]: input Channel::[0, 2], state:[1 2 1 2 1 2 tensor(0, device='cuda:0') 2 1 2 tensor(0, device='cuda:0') 2
 1 2 1 2 1 2 1 2 1 2 tensor(0, device='cuda:0') 2 0 2 1 2 1 2 1 2 0 2 1 2
 0 2 0 2], reward = -5.217031
mean of reward_transmit_list: -5.59232444634896
step in collision 5114.0 5326.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[0 2 0 2 0 2 0 2 1 2 tensor(0, device='cuda:0') 2 1 2
 tensor(0, device='cuda:0') 2 0 2 0 2 0 2 1 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 1 2
 0 2], reward = -7.458134171671
mean of reward_transmit_list: -5.607371298972525
step in collision 5116.0 5326.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[tensor(0, device='cuda:0'), 2], state:[0 2 1 2 0 2 1 2 0 2 0 2 0 2 1 2 0 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 0 2 0 2 0 2 1 2 1 2
 tensor(0, device='cuda:0') 2 1 2 0 2 tensor(0, device='cuda:0') 2], reward = -7.7123207545039
mean of reward_transmit_list: -5.624210894616775
step in collision 5133.0 5328.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[0 2 0 2 1 2 0 2 1 2 1 2 1 2 0 2 1 2 1 2 1 2 1 2 0 2 0 2
 tensor(1, device='cuda:0') 2 0 2 0 2 0 2 1 2 0 2], reward = -6.5132155990000005
mean of reward_transmit_list: -5.6312664875087055
step in collision 5322.0 5345.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[tensor(0, device='cuda:0'), 2], state:[0 2 0 2 1 2 tensor(0, device='cuda:0') 2 1 2 tensor(0, device='cuda:0') 2
 0 2 0 2 0 2 1 2 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 1 2 0 2 1 2
 tensor(0, device='cuda:0') 2], reward = -7.175704635190001
mean of reward_transmit_list: -5.643427417805408
step in collision 5329.0 5534.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[1 2 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 1 2 1 2
 tensor(0, device='cuda:0') 2 0 2 1 2 1 2 1 2 0 2 1 2 0 2 0 2 1 2 0 2 0 2
 0 2], reward = -6.12579511
mean of reward_transmit_list: -5.647195915400679
step in collision 5329.0 5541.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[1 2 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2
 tensor(0, device='cuda:0') 2 1 2 0 2 1 2 1 2 0 2 0 2
 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 0 2 0 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2], reward = -7.458134171671
mean of reward_transmit_list: -5.661234196457038
step in collision 5331.0 5541.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[tensor(0, device='cuda:0'), 2], state:[0 2 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2 0 2 0 2
 1 2 1 2 tensor(0, device='cuda:0') 2 1 2 0 2 tensor(0, device='cuda:0') 2
 1 2 0 2 0 2 tensor(0, device='cuda:0') 2 0 2 tensor(0, device='cuda:0') 2
 0 2 tensor(0, device='cuda:0') 2], reward = -8.14697981114816
mean of reward_transmit_list: -5.680355316570046
step in collision 5352.0 5543.0
[in reward_transmit]: input Channel::[1, 2], state:[0 2 1 2 0 2 1 2 1 2 1 2 0 2 1 2 1 2 1 2 1 2 0 2 0 2
 tensor(1, device='cuda:0') 2 0 2 0 2 0 2 1 2 0 2 1 2], reward = -6.861894039100001
mean of reward_transmit_list: -5.689374696131344
step in collision 5534.0 5564.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[1, 2], state:[tensor(0, device='cuda:0') 2 1 2 1 2 1 2 1 2 1 2
 tensor(0, device='cuda:0') 2 0 2 1 2 1 2 1 2 0 2 1 2 0 2 0 2 1 2 0 2 0 2
 0 2 1 2], reward = -6.861894039100001
mean of reward_transmit_list: -5.698257418426561
step in collision 5541.0 5746.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[tensor(0, device='cuda:0') 2 1 2 tensor(0, device='cuda:0') 2 0 2 0 2 0 2
 1 2 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 1 2 0 2 1 2
 tensor(0, device='cuda:0') 2 1 2 0 2 0 2], reward = -7.175704635190001
mean of reward_transmit_list: -5.709366044116511
step in collision 5543.0 5753.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[tensor(0, device='cuda:0') 2 0 2 tensor(0, device='cuda:0') 2 1 2 0 2 1 2
 1 2 0 2 0 2 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 0 2 0 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2 1 2 0 2], reward = -7.458134171671
mean of reward_transmit_list: -5.722416552531096
step in collision 5544.0 5755.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[0 2 0 2 1 2 1 2 tensor(0, device='cuda:0') 2 1 2 0 2
 tensor(0, device='cuda:0') 2 1 2 0 2 0 2 tensor(0, device='cuda:0') 2 0 2
 tensor(0, device='cuda:0') 2 0 2 tensor(0, device='cuda:0') 2 1 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2], reward = -7.94108867905351
mean of reward_transmit_list: -5.738851160875707
step in collision 5567.0 5756.0
[in reward_transmit]: input Channel::[1, 2], state:[1 2 0 2 1 2 1 2 1 2 0 2 1 2 1 2 1 2 1 2 0 2 0 2
 tensor(1, device='cuda:0') 2 0 2 0 2 0 2 1 2 0 2 1 2 1 2], reward = -7.175704635190001
mean of reward_transmit_list: -5.7494162599515475
step in collision 5746.0 5779.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[1, 2], state:[1 2 tensor(0, device='cuda:0') 2 0 2 0 2 0 2 1 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 1 2 0 2 1 2
 tensor(0, device='cuda:0') 2 1 2 0 2 0 2 1 2], reward = -6.12579511
mean of reward_transmit_list: -5.752163550827813
step in collision 5755.0 5958.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[tensor(0, device='cuda:0') 2 1 2 0 2 1 2 1 2 0 2 0 2
 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 0 2 0 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2 1 2 0 2 1 2
 0 2], reward = -7.175704635190001
mean of reward_transmit_list: -5.762479065931887
step in collision 5757.0 5967.0
[in reward_transmit]: input Channel::[tensor(0, device='cuda:0'), 2], state:[tensor(0, device='cuda:0') 2 0 2 1 2 1 2 1 2 0 2 1 2 0 2 0 2 1 2 0 2 0 2
 0 2 1 2 1 2 0 2 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2], reward = -7.458134171671
mean of reward_transmit_list: -5.774678023527132
step in collision 5758.0 5969.0
[in reward_transmit]: input Channel::[1, 2], state:[0 2 1 2 1 2 tensor(0, device='cuda:0') 2 1 2 0 2
 tensor(0, device='cuda:0') 2 1 2 0 2 0 2 tensor(0, device='cuda:0') 2 0 2
 tensor(0, device='cuda:0') 2 0 2 tensor(0, device='cuda:0') 2 1 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2 1 2], reward = -4.68559
mean of reward_transmit_list: -5.766898823359082
step in collision 5779.0 5970.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[1 2 1 2 1 2 0 2 1 2 1 2 1 2 1 2 0 2 0 2 tensor(1, device='cuda:0') 2 0 2
 0 2 0 2 1 2 0 2 1 2 1 2 1 2 0 2], reward = -5.6953279000000006
mean of reward_transmit_list: -5.7663912281579535
step in collision 5959.0 5991.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[0 2 0 2 0 2 1 2 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 1 2 0 2 1 2
 tensor(0, device='cuda:0') 2 1 2 0 2 0 2 1 2 1 2 0 2], reward = -6.861894039100001
mean of reward_transmit_list: -5.774106036685715
step in collision 5968.0 6171.0
[in reward_transmit]: input Channel::[1, 2], state:[1 2 0 2 1 2 1 2 0 2 0 2 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 0 2 0 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2 1 2 0 2 1 2
 0 2 1 2], reward = -6.12579511
mean of reward_transmit_list: -5.776565400834765
step in collision 5969.0 6180.0
[in reward_transmit]: input Channel::[1, 2], state:[0 2 1 2 1 2 1 2 0 2 1 2 0 2 0 2 1 2 0 2 0 2 0 2 1 2 1 2 0 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 1 2], reward = -5.6953279000000006
mean of reward_transmit_list: -5.776001251523414
step in collision 5970.0 6181.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[tensor(0, device='cuda:0') 2 1 2 0 2 tensor(0, device='cuda:0') 2 1 2 0 2
 0 2 tensor(0, device='cuda:0') 2 0 2 tensor(0, device='cuda:0') 2 0 2
 tensor(0, device='cuda:0') 2 1 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 0 2 1 2 1 2 tensor(0, device='cuda:0') 2 0 2], reward = -7.94108867905351
mean of reward_transmit_list: -5.790932888954655
step in collision 5993.0 6182.0
[in reward_transmit]: input Channel::[tensor(1, device='cuda:0'), 2], state:[1 2 1 2 0 2 1 2 1 2 1 2 1 2 0 2 0 2 tensor(1, device='cuda:0') 2 0 2 0 2
 0 2 1 2 0 2 1 2 1 2 1 2 0 2 tensor(1, device='cuda:0') 2], reward = -7.175704635190001
mean of reward_transmit_list: -5.8004176269425685
step in collision 6171.0 6205.0
[in reward_transmit]: input Channel::[1, 2], state:[0 2 0 2 1 2 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 1 2 0 2 1 2
 tensor(0, device='cuda:0') 2 1 2 0 2 0 2 1 2 1 2 0 2 1 2], reward = -6.5132155990000005
mean of reward_transmit_list: -5.805266592738878
step in collision 6180.0 6383.0
[in reward_transmit]: input Channel::[1, 2], state:[0 2 1 2 1 2 0 2 0 2 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 0 2 0 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2 1 2 0 2 1 2
 0 2 1 2 1 2], reward = -6.12579511
mean of reward_transmit_list: -5.807432325963615
step in collision 6181.0 6392.0
[in reward_transmit]: input Channel::[1, 2], state:[1 2 1 2 1 2 0 2 1 2 0 2 0 2 1 2 0 2 0 2 0 2 1 2 1 2 0 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 1 2 1 2], reward = -6.12579511
mean of reward_transmit_list: -5.809568988943725
step in collision 6182.0 6393.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[1 2 0 2 0 2 tensor(0, device='cuda:0') 2 0 2 tensor(0, device='cuda:0') 2
 0 2 tensor(0, device='cuda:0') 2 1 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 0 2 1 2 1 2 tensor(0, device='cuda:0') 2 0 2
 1 2 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2], reward = -7.94108867905351
mean of reward_transmit_list: -5.823779120211124
step in collision 6208.0 6394.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[1 2 1 2 0 2 0 2 tensor(1, device='cuda:0') 2 0 2 0 2 0 2 1 2 0 2 1 2 1 2
 1 2 0 2 tensor(1, device='cuda:0') 2 1 2 0 2 0 2 0 2 0 2], reward = -6.861894039100001
mean of reward_transmit_list: -5.830654053448798
step in collision 6387.0 6420.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 1 2 0 2 1 2
 tensor(0, device='cuda:0') 2 1 2 0 2 0 2 1 2 1 2 0 2 1 2 1 2
 tensor(0, device='cuda:0') 2 0 2], reward = -6.5132155990000005
mean of reward_transmit_list: -5.835144589932688
step in collision 6394.0 6599.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[1, 2], state:[1 2 1 2 0 2 1 2 0 2 0 2 1 2 0 2 0 2 0 2 1 2 1 2 0 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 1 2 1 2 1 2], reward = -6.12579511
mean of reward_transmit_list: -5.837044266534435
step in collision 6394.0 6606.0
[in reward_transmit]: input Channel::[0, 2], state:[0 2 0 2 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 0 2 0 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2 1 2 0 2 1 2
 0 2 1 2 1 2 1 2 tensor(0, device='cuda:0') 2 0 2], reward = -7.175704635190001
mean of reward_transmit_list: -5.8457368663309
step in collision 6395.0 6606.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[0 2 1 2 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2 1 2
 tensor(0, device='cuda:0') 2 0 2 0 2 tensor(0, device='cuda:0') 2 0 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 0 2 0 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2], reward = -8.49905364703001
mean of reward_transmit_list: -5.862855039109603
step in collision 6434.0 6607.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[tensor(1, device='cuda:0') 2 0 2 0 2 0 2 1 2 0 2 1 2 1 2 1 2 0 2
 tensor(1, device='cuda:0') 2 1 2 0 2 0 2 0 2 0 2 1 2 0 2 0 2 0 2], reward = -7.175704635190001
mean of reward_transmit_list: -5.87127074164858
step in collision 6602.0 6646.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[tensor(0, device='cuda:0'), 2], state:[1 2 1 2 1 2 0 2 0 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 0 2 1 2 0 2 1 2 0 2 1 2 1 2 1 2
 tensor(0, device='cuda:0') 2 0 2 1 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2], reward = -6.861894039100001
mean of reward_transmit_list: -5.877580444180118
step in collision 6609.0 6814.0
[in reward_transmit]: input Channel::[tensor(0, device='cuda:0'), 2], state:[1 2 1 2 1 2 0 2 1 2 tensor(0, device='cuda:0') 2 1 2 0 2 0 2 1 2 1 2 0 2
 1 2 1 2 tensor(0, device='cuda:0') 2 0 2 1 2 0 2 0 2
 tensor(0, device='cuda:0') 2], reward = -6.5132155990000005
mean of reward_transmit_list: -5.881603451489104
step in collision 6609.0 6821.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[1 2 0 2 0 2 0 2 1 2 1 2 0 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 1 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 0 2 0 2 0 2], reward = -7.458134171671
mean of reward_transmit_list: -5.8915187390374175
step in collision 6611.0 6821.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[tensor(0, device='cuda:0'), 2], state:[tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2 1 2
 tensor(0, device='cuda:0') 2 0 2 0 2 tensor(0, device='cuda:0') 2 0 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 0 2 0 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2 1 2
 tensor(0, device='cuda:0') 2], reward = -8.49905364703001
mean of reward_transmit_list: -5.907815832212371
step in collision 6647.0 6823.0
[in reward_transmit]: input Channel::[1, 2], state:[0 2 0 2 0 2 1 2 0 2 1 2 1 2 1 2 0 2 tensor(1, device='cuda:0') 2 1 2 0 2
 0 2 0 2 0 2 1 2 0 2 0 2 0 2 1 2], reward = -5.6953279000000006
mean of reward_transmit_list: -5.9064960313911765
step in collision 6814.0 6859.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[0 2 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2 1 2 0 2
 1 2 0 2 1 2 1 2 1 2 tensor(0, device='cuda:0') 2 0 2 1 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 1 2
 tensor(0, device='cuda:0') 2 0 2 0 2], reward = -7.458134171671
mean of reward_transmit_list: -5.91607404460278
step in collision 6824.0 7026.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[0 2 1 2 1 2 0 2 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 1 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2 0 2 0 2 1 2
 0 2 0 2], reward = -7.458134171671
mean of reward_transmit_list: -5.925534536179886
step in collision 6825.0 7036.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[1 2 0 2 0 2 1 2 1 2 0 2 1 2 1 2 tensor(0, device='cuda:0') 2 0 2 1 2 0 2
 0 2 tensor(0, device='cuda:0') 2 1 2 0 2 tensor(0, device='cuda:0') 2 0 2
 tensor(0, device='cuda:0') 2 0 2], reward = -7.458134171671
mean of reward_transmit_list: -5.93487965590849
step in collision 6826.0 7037.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[tensor(0, device='cuda:0'), 2], state:[1 2 tensor(0, device='cuda:0') 2 0 2 0 2 tensor(0, device='cuda:0') 2 0 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 0 2 0 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2 1 2
 tensor(0, device='cuda:0') 2 1 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2], reward = -8.332281830033345
mean of reward_transmit_list: -5.949409366054701
step in collision 6861.0 7038.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[0 2 1 2 1 2 1 2 0 2 tensor(1, device='cuda:0') 2 1 2 0 2 0 2 0 2 0 2 1 2
 0 2 0 2 0 2 1 2 1 2 0 2 0 2 0 2], reward = -7.175704635190001
mean of reward_transmit_list: -5.956796686953107
step in collision 7029.0 7073.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[1, 2], state:[1 2 1 2 0 2 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 1 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2 0 2 0 2 1 2
 0 2 0 2 1 2], reward = -5.6953279000000006
mean of reward_transmit_list: -5.955231005594106
step in collision 7037.0 7241.0
[in reward_transmit]: input Channel::[1, 2], state:[0 2 0 2 1 2 1 2 0 2 1 2 1 2 tensor(0, device='cuda:0') 2 0 2 1 2 0 2 0 2
 tensor(0, device='cuda:0') 2 1 2 0 2 tensor(0, device='cuda:0') 2 0 2
 tensor(0, device='cuda:0') 2 0 2 1 2], reward = -5.217031
mean of reward_transmit_list: -5.95083695794176
step in collision 7038.0 7249.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[0 2 1 2 0 2 1 2 1 2 1 2 tensor(0, device='cuda:0') 2 0 2 1 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 1 2
 tensor(0, device='cuda:0') 2 0 2 0 2 1 2 0 2 tensor(0, device='cuda:0') 2
 0 2 0 2], reward = -7.458134171671
mean of reward_transmit_list: -5.959755876366193
step in collision 7040.0 7250.0
[in reward_transmit]: input Channel::[1, 2], state:[tensor(0, device='cuda:0') 2 0 2 0 2 tensor(0, device='cuda:0') 2 0 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 0 2 0 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2 1 2
 tensor(0, device='cuda:0') 2 1 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 1 2], reward = -2.71
mean of reward_transmit_list: -5.940639665328746
step in collision 7073.0 7252.0
[in reward_transmit]: input Channel::[tensor(1, device='cuda:0'), 2], state:[1 2 1 2 1 2 0 2 tensor(1, device='cuda:0') 2 1 2 0 2 0 2 0 2 0 2 1 2 0 2
 0 2 0 2 1 2 1 2 0 2 0 2 0 2 tensor(1, device='cuda:0') 2], reward = -6.12579511
mean of reward_transmit_list: -5.94172244570694
step in collision 7241.0 7285.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[0 2 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 1 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2 0 2 0 2 1 2
 0 2 0 2 1 2 1 2 0 2], reward = -7.458134171671
mean of reward_transmit_list: -5.950538792950917
step in collision 7250.0 7453.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[1 2 0 2 1 2 1 2 tensor(0, device='cuda:0') 2 0 2 1 2 0 2 0 2
 tensor(0, device='cuda:0') 2 1 2 0 2 tensor(0, device='cuda:0') 2 0 2
 tensor(0, device='cuda:0') 2 0 2 1 2 1 2 0 2 0 2], reward = -7.458134171671
mean of reward_transmit_list: -5.959253217105369
step in collision 7252.0 7462.0
[in reward_transmit]: input Channel::[1, 2], state:[1 2 0 2 1 2 1 2 1 2 tensor(0, device='cuda:0') 2 0 2 1 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 1 2
 tensor(0, device='cuda:0') 2 0 2 0 2 1 2 0 2 tensor(0, device='cuda:0') 2
 0 2 0 2 1 2], reward = -5.6953279000000006
mean of reward_transmit_list: -5.957736404938096
step in collision 7252.0 7464.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[tensor(0, device='cuda:0'), 2], state:[1 2 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 1 2 1 2
 tensor(0, device='cuda:0') 2 0 2 0 2 0 2 tensor(0, device='cuda:0') 2 0 2
 0 2 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2 0 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2
 tensor(0, device='cuda:0') 2], reward = -8.332281830033345
mean of reward_transmit_list: -5.97130523593864
step in collision 7300.0 7464.0
[in reward_transmit]: input Channel::[tensor(1, device='cuda:0'), 2], state:[1 2 1 2 0 2 tensor(1, device='cuda:0') 2 1 2 0 2 0 2 0 2 0 2 1 2 0 2 0 2
 0 2 1 2 1 2 0 2 0 2 0 2 tensor(1, device='cuda:0') 2
 tensor(1, device='cuda:0') 2], reward = -6.12579511
mean of reward_transmit_list: -5.972183019313988
step in collision 7453.0 7512.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 1 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2 0 2 0 2 1 2
 0 2 0 2 1 2 1 2 0 2 1 2 0 2 0 2], reward = -7.175704635190001
mean of reward_transmit_list: -5.97898257646583
step in collision 7464.0 7665.0
[in reward_transmit]: input Channel::[1, 2], state:[0 2 1 2 1 2 1 2 tensor(0, device='cuda:0') 2 0 2 1 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 1 2
 tensor(0, device='cuda:0') 2 0 2 0 2 1 2 0 2 tensor(0, device='cuda:0') 2
 0 2 0 2 1 2 1 2], reward = -5.6953279000000006
mean of reward_transmit_list: -5.977389010867706
step in collision 7464.0 7676.0
[in reward_transmit]: input Channel::[0, 2], state:[1 2 1 2 tensor(0, device='cuda:0') 2 0 2 1 2 0 2 0 2
 tensor(0, device='cuda:0') 2 1 2 0 2 tensor(0, device='cuda:0') 2 0 2
 tensor(0, device='cuda:0') 2 0 2 1 2 1 2 0 2 0 2 1 2 0 2], reward = -7.458134171671
mean of reward_transmit_list: -5.985661330201803
step in collision 7465.0 7676.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[tensor(0, device='cuda:0'), 2], state:[tensor(0, device='cuda:0') 2 1 2 1 2 tensor(0, device='cuda:0') 2 0 2 0 2
 0 2 tensor(0, device='cuda:0') 2 0 2 0 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 0 2 0 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 0 2 tensor(0, device='cuda:0') 2 1 2
 tensor(0, device='cuda:0') 2], reward = -8.332281830033345
mean of reward_transmit_list: -5.998698110756422
step in collision 7513.0 7677.0
[in reward_transmit]: input Channel::[1, 2], state:[1 2 0 2 tensor(1, device='cuda:0') 2 1 2 0 2 0 2 0 2 0 2 1 2 0 2 0 2 0 2
 1 2 1 2 0 2 0 2 0 2 tensor(1, device='cuda:0') 2
 tensor(1, device='cuda:0') 2 1 2], reward = -6.12579511
mean of reward_transmit_list: -5.999400304122409
step in collision 7665.0 7725.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[tensor(0, device='cuda:0'), 2], state:[1 2 1 2 tensor(0, device='cuda:0') 2 0 2 1 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 1 2 tensor(0, device='cuda:0') 2 0 2 0 2 1 2
 0 2 tensor(0, device='cuda:0') 2 0 2 0 2 1 2 1 2 1 2
 tensor(0, device='cuda:0') 2], reward = -7.175704635190001
mean of reward_transmit_list: -6.0058635147326696
step in collision 7677.0 7877.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[tensor(0, device='cuda:0') 2 0 2 1 2 0 2 0 2 tensor(0, device='cuda:0') 2
 1 2 0 2 tensor(0, device='cuda:0') 2 0 2 tensor(0, device='cuda:0') 2 0 2
 1 2 1 2 0 2 0 2 1 2 0 2 1 2 0 2], reward = -7.7123207545039
mean of reward_transmit_list: -6.015188417682239
step in collision 7678.0 7889.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[tensor(0, device='cuda:0'), 2], state:[1 2 1 2 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2 0 2
 0 2 1 2 0 2 0 2 1 2 1 2 0 2 1 2 0 2 0 2 1 2 tensor(0, device='cuda:0') 2
 0 2 tensor(0, device='cuda:0') 2], reward = -7.458134171671
mean of reward_transmit_list: -6.0230305141713085
step in collision 7679.0 7890.0
[in reward_transmit]: input Channel::[1, 2], state:[1 2 1 2 tensor(0, device='cuda:0') 2 0 2 0 2 0 2
 tensor(0, device='cuda:0') 2 0 2 0 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 0 2 0 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 0 2 tensor(0, device='cuda:0') 2 1 2
 tensor(0, device='cuda:0') 2 1 2], reward = -3.439
mean of reward_transmit_list: -6.009062781662275
step in collision 7725.0 7891.0
[in reward_transmit]: input Channel::[tensor(1, device='cuda:0'), 2], state:[0 2 tensor(1, device='cuda:0') 2 1 2 0 2 0 2 0 2 0 2 1 2 0 2 0 2 0 2 1 2
 1 2 0 2 0 2 0 2 tensor(1, device='cuda:0') 2 tensor(1, device='cuda:0') 2
 1 2 tensor(1, device='cuda:0') 2], reward = -6.12579511
mean of reward_transmit_list: -6.0096903748253805
step in collision 7877.0 7937.0
[in reward_transmit]: input Channel::[1, 2], state:[1 2 tensor(0, device='cuda:0') 2 0 2 1 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 1 2 tensor(0, device='cuda:0') 2 0 2 0 2 1 2
 0 2 tensor(0, device='cuda:0') 2 0 2 0 2 1 2 1 2 1 2
 tensor(0, device='cuda:0') 2 1 2], reward = -5.6953279000000006
mean of reward_transmit_list: -6.008009292072303
step in collision 7889.0 8089.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[1, 2], state:[1 2 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2 0 2 0 2
 1 2 0 2 0 2 1 2 1 2 0 2 1 2 0 2 0 2 1 2 tensor(0, device='cuda:0') 2 0 2
 tensor(0, device='cuda:0') 2 1 2], reward = -5.217031
mean of reward_transmit_list: -6.003801960731493
step in collision 7891.0 8101.0
[in reward_transmit]: input Channel::[0, 2], state:[0 2 0 2 tensor(0, device='cuda:0') 2 1 2 0 2 tensor(0, device='cuda:0') 2
 0 2 tensor(0, device='cuda:0') 2 0 2 1 2 1 2 0 2 0 2 1 2 0 2 1 2 0 2 1 2
 0 2 0 2], reward = -7.7123207545039
mean of reward_transmit_list: -6.012841742709124
step in collision 7892.0 8103.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[tensor(0, device='cuda:0'), 2], state:[tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2 0 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2
 tensor(0, device='cuda:0') 2 1 2 tensor(0, device='cuda:0') 2 1 2 1 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2], reward = -8.332281830033345
mean of reward_transmit_list: -6.025049322116094
step in collision 7945.0 8104.0
[in reward_transmit]: input Channel::[1, 2], state:[tensor(1, device='cuda:0') 2 1 2 0 2 0 2 0 2 0 2 1 2 0 2 0 2 0 2 1 2 1 2
 0 2 0 2 0 2 tensor(1, device='cuda:0') 2 tensor(1, device='cuda:0') 2 1 2
 tensor(1, device='cuda:0') 2 1 2], reward = -6.5132155990000005
mean of reward_transmit_list: -6.027605166497684
step in collision 8089.0 8157.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[0 2 1 2 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 1 2
 tensor(0, device='cuda:0') 2 0 2 0 2 1 2 0 2 tensor(0, device='cuda:0') 2
 0 2 0 2 1 2 1 2 1 2 tensor(0, device='cuda:0') 2 1 2 1 2 0 2], reward = -7.175704635190001
mean of reward_transmit_list: -6.033584851230457
step in collision 8102.0 8301.0
[in reward_transmit]: input Channel::[1, 2], state:[tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2 0 2 0 2 1 2
 0 2 0 2 1 2 1 2 0 2 1 2 0 2 0 2 1 2 tensor(0, device='cuda:0') 2 0 2
 tensor(0, device='cuda:0') 2 1 2 1 2], reward = -5.217031
mean of reward_transmit_list: -6.029354002260351
step in collision 8103.0 8314.0
[in reward_transmit]: input Channel::[1, 2], state:[0 2 tensor(0, device='cuda:0') 2 1 2 0 2 tensor(0, device='cuda:0') 2 0 2
 tensor(0, device='cuda:0') 2 0 2 1 2 1 2 0 2 0 2 1 2 0 2 1 2 0 2 1 2 0 2
 0 2 1 2], reward = -5.217031
mean of reward_transmit_list: -6.025166770289935
step in collision 8104.0 8315.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[tensor(0, device='cuda:0'), 2], state:[0 2 0 2 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2
 tensor(0, device='cuda:0') 2 1 2 tensor(0, device='cuda:0') 2 1 2 1 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 1 2 tensor(0, device='cuda:0') 2], reward = -8.14697981114816
mean of reward_transmit_list: -6.036047862807157
step in collision 8158.0 8316.0
[in reward_transmit]: input Channel::[tensor(1, device='cuda:0'), 2], state:[1 2 0 2 0 2 0 2 0 2 1 2 0 2 0 2 0 2 1 2 1 2 0 2 0 2 0 2
 tensor(1, device='cuda:0') 2 tensor(1, device='cuda:0') 2 1 2
 tensor(1, device='cuda:0') 2 1 2 tensor(1, device='cuda:0') 2], reward = -6.5132155990000005
mean of reward_transmit_list: -6.038482392073448
step in collision 8301.0 8370.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 1 2
 tensor(0, device='cuda:0') 2 0 2 0 2 1 2 0 2 tensor(0, device='cuda:0') 2
 0 2 0 2 1 2 1 2 1 2 tensor(0, device='cuda:0') 2 1 2 1 2 0 2 1 2 0 2], reward = -7.175704635190001
mean of reward_transmit_list: -6.044255093815155
step in collision 8315.0 8513.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[tensor(0, device='cuda:0'), 2], state:[1 2 0 2 tensor(0, device='cuda:0') 2 0 2 tensor(0, device='cuda:0') 2 0 2
 1 2 1 2 0 2 0 2 1 2 0 2 1 2 0 2 1 2 0 2 0 2 1 2 1 2
 tensor(0, device='cuda:0') 2], reward = -7.175704635190001
mean of reward_transmit_list: -6.04996948543826
step in collision 8317.0 8527.0
[in reward_transmit]: input Channel::[tensor(0, device='cuda:0'), 2], state:[0 2 1 2 0 2 0 2 1 2 1 2 0 2 1 2 0 2 0 2 1 2 tensor(0, device='cuda:0') 2
 0 2 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 0 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2], reward = -7.175704635190001
mean of reward_transmit_list: -6.055626445989777
step in collision 8318.0 8529.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[0 2 tensor(0, device='cuda:0') 2 1 2 tensor(0, device='cuda:0') 2 1 2 1 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 1 2 tensor(0, device='cuda:0') 2 1 2 0 2
 tensor(0, device='cuda:0') 2 0 2], reward = -7.94108867905351
mean of reward_transmit_list: -6.065053757155096
step in collision 8373.0 8530.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[0 2 0 2 0 2 1 2 0 2 0 2 0 2 1 2 1 2 0 2 0 2 0 2
 tensor(1, device='cuda:0') 2 tensor(1, device='cuda:0') 2 1 2
 tensor(1, device='cuda:0') 2 1 2 tensor(1, device='cuda:0') 2 1 2 0 2], reward = -6.5132155990000005
mean of reward_transmit_list: -6.067283418059796
step in collision 8514.0 8585.0
[in reward_transmit]: input Channel::[1, 2], state:[tensor(0, device='cuda:0') 2 1 2 tensor(0, device='cuda:0') 2 0 2 0 2 1 2
 0 2 tensor(0, device='cuda:0') 2 0 2 0 2 1 2 1 2 1 2
 tensor(0, device='cuda:0') 2 1 2 1 2 0 2 1 2 0 2 1 2], reward = -6.12579511
mean of reward_transmit_list: -6.067573079901084
step in collision 8527.0 8726.0
[in reward_transmit]: input Channel::[1, 2], state:[0 2 tensor(0, device='cuda:0') 2 0 2 tensor(0, device='cuda:0') 2 0 2 1 2
 1 2 0 2 0 2 1 2 0 2 1 2 0 2 1 2 0 2 0 2 1 2 1 2
 tensor(0, device='cuda:0') 2 1 2], reward = -5.6953279000000006
mean of reward_transmit_list: -6.065739359803049
step in collision 8529.0 8739.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[0 2 0 2 1 2 1 2 0 2 1 2 0 2 0 2 1 2 tensor(0, device='cuda:0') 2 0 2
 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 0 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 1 2 0 2], reward = -7.175704635190001
mean of reward_transmit_list: -6.0711803660549455
step in collision 8531.0 8741.0
[in reward_transmit]: input Channel::[1, 2], state:[tensor(0, device='cuda:0') 2 1 2 tensor(0, device='cuda:0') 2 1 2 1 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 1 2 tensor(0, device='cuda:0') 2 1 2 0 2
 tensor(0, device='cuda:0') 2 0 2 1 2], reward = -4.68559
mean of reward_transmit_list: -6.0644213886595555
step in collision 8585.0 8743.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[1 2 0 2 0 2 0 2 1 2 1 2 0 2 0 2 0 2 tensor(1, device='cuda:0') 2
 tensor(1, device='cuda:0') 2 1 2 tensor(1, device='cuda:0') 2 1 2
 tensor(1, device='cuda:0') 2 1 2 0 2 1 2 0 2 0 2], reward = -6.12579511
mean of reward_transmit_list: -6.064719319345674
step in collision 8728.0 8797.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[tensor(0, device='cuda:0'), 2], state:[tensor(0, device='cuda:0') 2 0 2 0 2 1 2 0 2 tensor(0, device='cuda:0') 2
 0 2 0 2 1 2 1 2 1 2 tensor(0, device='cuda:0') 2 1 2 1 2 0 2 1 2 0 2 1 2
 1 2 tensor(0, device='cuda:0') 2], reward = -6.861894039100001
mean of reward_transmit_list: -6.0685704049483515
step in collision 8740.0 8940.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[1, 2], state:[0 2 1 2 1 2 0 2 1 2 0 2 0 2 1 2 tensor(0, device='cuda:0') 2 0 2
 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 0 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 1 2 0 2 1 2], reward = -6.12579511
mean of reward_transmit_list: -6.068845523722638
step in collision 8743.0 8952.0
[in reward_transmit]: input Channel::[0, 2], state:[tensor(0, device='cuda:0') 2 0 2 1 2 1 2 0 2 0 2 1 2 0 2 1 2 0 2 1 2 0 2
 0 2 1 2 1 2 tensor(0, device='cuda:0') 2 1 2 1 2
 tensor(0, device='cuda:0') 2 0 2], reward = -6.861894039100001
mean of reward_transmit_list: -6.0726400142268355
step in collision 8743.0 8955.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[1 2 1 2 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 1 2 tensor(0, device='cuda:0') 2 1 2 0 2
 tensor(0, device='cuda:0') 2 0 2 1 2 1 2 0 2 0 2], reward = -7.7123207545039
mean of reward_transmit_list: -6.080448017751965
step in collision 8799.0 8955.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[0 2 0 2 1 2 1 2 0 2 0 2 0 2 tensor(1, device='cuda:0') 2
 tensor(1, device='cuda:0') 2 1 2 tensor(1, device='cuda:0') 2 1 2
 tensor(1, device='cuda:0') 2 1 2 0 2 1 2 0 2 0 2 1 2 0 2], reward = -6.12579511
mean of reward_transmit_list: -6.080662932881102
step in collision 8941.0 9011.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[1, 2], state:[1 2 1 2 0 2 1 2 0 2 0 2 1 2 tensor(0, device='cuda:0') 2 0 2
 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 0 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 1 2 0 2 1 2 1 2], reward = -6.5132155990000005
mean of reward_transmit_list: -6.082703275645814
step in collision 8955.0 9153.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[tensor(0, device='cuda:0') 2 0 2 0 2 1 2 1 2 1 2
 tensor(0, device='cuda:0') 2 1 2 1 2 0 2 1 2 0 2 1 2 1 2
 tensor(0, device='cuda:0') 2 1 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2], reward = -6.861894039100001
mean of reward_transmit_list: -6.08636144824419
step in collision 8956.0 9167.0
[in reward_transmit]: input Channel::[0, 2], state:[1 2 1 2 0 2 0 2 1 2 0 2 1 2 0 2 1 2 0 2 0 2 1 2 1 2
 tensor(0, device='cuda:0') 2 1 2 1 2 tensor(0, device='cuda:0') 2 0 2 1 2
 0 2], reward = -6.5132155990000005
mean of reward_transmit_list: -6.0883560938084695
step in collision 8956.0 9168.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[tensor(0, device='cuda:0'), 2], state:[tensor(0, device='cuda:0') 2 1 2 0 2 tensor(0, device='cuda:0') 2 0 2 1 2
 1 2 0 2 0 2 1 2 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 0 2 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2], reward = -8.14697981114816
mean of reward_transmit_list: -6.097931087842608
step in collision 9021.0 9168.0
[in reward_transmit]: input Channel::[1, 2], state:[0 2 1 2 1 2 0 2 0 2 0 2 tensor(1, device='cuda:0') 2
 tensor(1, device='cuda:0') 2 1 2 tensor(1, device='cuda:0') 2 1 2
 tensor(1, device='cuda:0') 2 1 2 0 2 1 2 0 2 0 2 1 2 0 2 1 2], reward = -7.175704635190001
mean of reward_transmit_list: -6.102920780191438
step in collision 9153.0 9233.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[0 2 1 2 0 2 0 2 1 2 tensor(0, device='cuda:0') 2 0 2
 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 0 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 1 2 0 2 1 2 1 2 1 2 0 2], reward = -6.861894039100001
mean of reward_transmit_list: -6.106418352813137
step in collision 9168.0 9365.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[tensor(0, device='cuda:0'), 2], state:[1 2 1 2 1 2 tensor(0, device='cuda:0') 2 1 2 1 2 0 2 1 2 0 2 1 2 1 2
 tensor(0, device='cuda:0') 2 1 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2 1 2 0 2
 tensor(0, device='cuda:0') 2], reward = -6.5132155990000005
mean of reward_transmit_list: -6.108284395226837
step in collision 9170.0 9380.0
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[tensor(0, device='cuda:0'), 2], state:[0 2 1 2 0 2 1 2 0 2 0 2 1 2 1 2 tensor(0, device='cuda:0') 2 1 2 1 2
 tensor(0, device='cuda:0') 2 0 2 1 2 0 2 1 2 0 2 0 2 0 2
 tensor(0, device='cuda:0') 2], reward = -7.175704635190001
mean of reward_transmit_list: -6.1131584602495
step in collision 9172.0 9382.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[1 2 1 2 0 2 0 2 1 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 0 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 1 2
 tensor(0, device='cuda:0') 2 0 2 0 2 0 2], reward = -8.14697981114816
mean of reward_transmit_list: -6.1224031027535855
step in collision 9237.0 9384.0
[in reward_transmit]: input Channel::[1, 2], state:[1 2 1 2 0 2 0 2 0 2 tensor(1, device='cuda:0') 2
 tensor(1, device='cuda:0') 2 1 2 tensor(1, device='cuda:0') 2 1 2
 tensor(1, device='cuda:0') 2 1 2 0 2 1 2 0 2 0 2 1 2 0 2 1 2 1 2], reward = -7.458134171671
mean of reward_transmit_list: -6.128447134739637
step in collision 9365.0 9449.0
[in reward_transmit]: input Channel::[1, 2], state:[1 2 0 2 0 2 1 2 tensor(0, device='cuda:0') 2 0 2
 tensor(0, device='cuda:0') 2 1 2 1 2 1 2 0 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 1 2 0 2 1 2 1 2 1 2 0 2 1 2], reward = -6.5132155990000005
mean of reward_transmit_list: -6.130180326020088
step in collision 9380.0 9577.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[tensor(0, device='cuda:0'), 2], state:[0 2 1 2 0 2 0 2 1 2 1 2 tensor(0, device='cuda:0') 2 1 2 1 2
 tensor(0, device='cuda:0') 2 0 2 1 2 0 2 1 2 0 2 0 2 0 2
 tensor(0, device='cuda:0') 2 1 2 tensor(0, device='cuda:0') 2], reward = -7.175704635190001
mean of reward_transmit_list: -6.134868775836994
step in collision 9385.0 9592.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[1 2 1 2 tensor(0, device='cuda:0') 2 1 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2 1 2 0 2
 tensor(0, device='cuda:0') 2 1 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 0 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 0 2], reward = -7.94108867905351
mean of reward_transmit_list: -6.142932257547782
step in collision 9390.0 9597.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[tensor(0, device='cuda:0'), 2], state:[0 2 0 2 1 2 tensor(0, device='cuda:0') 2 0 2 0 2
 tensor(0, device='cuda:0') 2 0 2 0 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2
 tensor(0, device='cuda:0') 2 0 2 tensor(0, device='cuda:0') 2], reward = -8.649148282327008
mean of reward_transmit_list: -6.154070995435689
step in collision 9466.0 9602.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[0, 2], state:[0 2 0 2 tensor(1, device='cuda:0') 2 tensor(1, device='cuda:0') 2 1 2
 tensor(1, device='cuda:0') 2 1 2 tensor(1, device='cuda:0') 2 1 2 0 2 1 2
 0 2 0 2 1 2 0 2 1 2 1 2 tensor(1, device='cuda:0') 2 0 2 0 2], reward = -5.6953279000000006
mean of reward_transmit_list: -6.1520411587302215
step in collision 9579.0 9678.0
[in reward_transmit]: input Channel::[1, 2], state:[0 2 0 2 1 2 tensor(0, device='cuda:0') 2 0 2 tensor(0, device='cuda:0') 2
 1 2 1 2 1 2 0 2 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 1 2 0 2 1 2 1 2 1 2 0 2 1 2 1 2], reward = -6.5132155990000005
mean of reward_transmit_list: -6.153632235559604
step in collision 9592.0 9791.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[tensor(0, device='cuda:0'), 2], state:[1 2 1 2 tensor(0, device='cuda:0') 2 1 2 1 2 tensor(0, device='cuda:0') 2
 0 2 1 2 0 2 1 2 0 2 0 2 0 2 tensor(0, device='cuda:0') 2 1 2
 tensor(0, device='cuda:0') 2 1 2 0 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2], reward = -7.175704635190001
mean of reward_transmit_list: -6.158115009242192
step in collision 9600.0 9804.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[tensor(0, device='cuda:0'), 2], state:[tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2 1 2 0 2
 tensor(0, device='cuda:0') 2 1 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 0 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 0 2 1 2 tensor(0, device='cuda:0') 2 0 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2], reward = -8.332281830033345
mean of reward_transmit_list: -6.167609187498923
step in collision 9606.0 9812.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[tensor(0, device='cuda:0'), 2], state:[tensor(0, device='cuda:0') 2 0 2 0 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2
 tensor(0, device='cuda:0') 2 0 2 tensor(0, device='cuda:0') 2 1 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 0 2 tensor(0, device='cuda:0') 2], reward = -8.649148282327008
mean of reward_transmit_list: -6.178398487911218
step in collision 9683.0 9818.0
[in reward_transmit]: input Channel::[1, 2], state:[0 2 tensor(1, device='cuda:0') 2 tensor(1, device='cuda:0') 2 1 2
 tensor(1, device='cuda:0') 2 1 2 tensor(1, device='cuda:0') 2 1 2 0 2 1 2
 0 2 0 2 1 2 0 2 1 2 1 2 tensor(1, device='cuda:0') 2 0 2 0 2 1 2], reward = -7.458134171671
mean of reward_transmit_list: -6.183938469226195
step in collision 9791.0 9895.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[tensor(0, device='cuda:0'), 2], state:[tensor(0, device='cuda:0') 2 1 2 1 2 1 2 0 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 1 2 0 2 1 2 1 2 1 2 0 2 1 2 1 2 1 2
 tensor(0, device='cuda:0') 2 0 2 0 2 tensor(0, device='cuda:0') 2], reward = -6.5132155990000005
mean of reward_transmit_list: -6.185357767199358
step in collision 9808.0 10003.0
[in reward_transmit]: input Channel::[1, 2], state:[1 2 tensor(0, device='cuda:0') 2 1 2 1 2 tensor(0, device='cuda:0') 2 0 2
 1 2 0 2 1 2 0 2 0 2 0 2 tensor(0, device='cuda:0') 2 1 2
 tensor(0, device='cuda:0') 2 1 2 0 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 1 2], reward = -5.6953279000000006
mean of reward_transmit_list: -6.183254634722108
step in collision 9812.0 10020.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[tensor(0, device='cuda:0'), 2], state:[1 2 0 2 tensor(0, device='cuda:0') 2 1 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 0 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 0 2 1 2 tensor(0, device='cuda:0') 2 0 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 1 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2], reward = -8.14697981114816
mean of reward_transmit_list: -6.191646622655552
step in collision 9820.0 10024.0
in [reward_wait] reward = 1
in [reward_wait] reward = 1
[in reward_transmit]: input Channel::[tensor(0, device='cuda:0'), 2], state:[tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 0 2 tensor(0, device='cuda:0') 2 0 2
 tensor(0, device='cuda:0') 2 1 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2 tensor(0, device='cuda:0') 2 0 2
 tensor(0, device='cuda:0') 2 1 2 tensor(0, device='cuda:0') 2
 tensor(0, device='cuda:0') 2], reward = -8.49905364703001
mean of reward_transmit_list: -6.201465375950763
step in collision 9897.0 10032.0
==> saving model...
==> saving model...
==> saving model...
==> saving model...
==> saving model...
==> total_time: 1040.0
==> total_time_channel: 0
==> channel time: 10109.0
==> throughput: 0.5935305173607677
